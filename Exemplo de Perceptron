import numpy as np

class Perceptron:
    """
    Implementação do classificador Perceptron de Rosenblatt.

    Parâmetros
    ----------
    eta : float
      Taxa de aprendizado (entre 0.0 e 1.0).
    n_iter : int
      Número máximo de passagens sobre o conjunto de treinamento (épocas).
    random_state : int
      Semente para o gerador de números aleatórios para inicialização de pesos.

    Atributos
    ---------
    w_ : array-1d
      Pesos após o treinamento. w_[0] é o bias.
    errors_ : list
      Número de classificações incorretas (erros) em cada época.
    """
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        """
        Ajusta o modelo aos dados de treinamento.

        Parâmetros
        ----------
        X : {array-like}, shape = [n_samples, n_features]
          Vetores de treinamento.
        y : array-like, shape = [n_samples]
          Valores alvo (rótulos). Usa-se 1 e -1.
        """
        rgen = np.random.RandomState(self.random_state)
        # Inicializa pesos com pequenos valores aleatórios, +1 para o bias
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.errors_ = []

        for i in range(self.n_iter):
            errors = 0
            for xi, target in zip(X, y):
                # Calcula a atualização dos pesos
                # update = eta * (target - predição)
                update = self.eta * (target - self.predict(xi))

                # Aplica a regra de atualização
                self.w_[1:] += update * xi  # Atualiza pesos das features
                self.w_[0] += update        # Atualiza o bias

                # Incrementa o contador de erros se a atualização não for zero
                errors += int(update != 0.0)

            self.errors_.append(errors)
            # --- Parada Antecipada ---
            # Se não houve erros nesta época, o modelo convergiu
            if errors == 0:
                print(f"Convergência alcançada na época {i+1}.")
                return self

        print(f"Máximo de épocas ({self.n_iter}) atingido sem convergência completa.")
        return self

    def net_input(self, X):
        """Calcula a entrada líquida (soma ponderada)"""
        # z = w · x + b
        # Garante que X seja pelo menos 1D para np.dot funcionar com amostra única
        X_proc = np.atleast_1d(X)
        return np.dot(X_proc, self.w_[1:]) + self.w_[0]

    def predict(self, X):
        """Retorna o rótulo da classe (1 ou -1) após a função degrau"""
        return np.where(self.net_input(X) >= 0.0, 1, -1)

# --- Exemplo de Uso (Problema AND, linearmente separável) ---
X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_and = np.array([-1, -1, -1, 1]) # Rótulos -1 (Falso) e 1 (Verdadeiro)

ppn = Perceptron(eta=0.1, n_iter=100) # Aumentei n_iter para garantir a parada antecipada
ppn.fit(X_and, y_and)

print("--- Teste do Perceptron (Porta Lógica AND) ---")
print(f"Pesos finais: {ppn.w_}")
print(f"Erros por época: {ppn.errors_}")
print(f"Predições: {ppn.predict(X_and)}")

# --- Exemplo de Uso (Problema XOR, não linearmente separável) ---
# O Perceptron NÃO deve convergir aqui
print("\n--- Teste do Perceptron (Porta Lógica XOR) ---")
X_xor_perc = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor_perc = np.array([-1, 1, 1, -1]) # Rótulos -1 e 1 para XOR

ppn_xor = Perceptron(eta=0.1, n_iter=100)
ppn_xor.fit(X_xor_perc, y_xor_perc)
print(f"Pesos finais (XOR): {ppn_xor.w_}")
print(f"Erros por época (XOR): {ppn_xor.errors_}") # Espera-se que os erros não cheguem a zero
print(f"Predições (XOR): {ppn_xor.predict(X_xor_perc)}")
